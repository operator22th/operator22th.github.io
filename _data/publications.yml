main:
  - title: "VisualMimic: Visual Humanoid Loco-Manipulation via Motion Tracking and Generation"
    authors: <strong>Shaofeng Yin*</strong>, Yanjie Ze*, Hong-Xing Yu, C. Karen Liu†, Jiajun Wu†
    conference_short: arxiv
    conference: arxiv, 2025.
    arxiv: https://arxiv.org/abs/2509.20322
    pdf: https://arxiv.org/pdf/2509.20322
    code: https://github.com/visualmimic/VisualMimic
    page: https://visualmimic.github.io/
    image: ./assets/pub/arxiv2025_visualmimic/kick_ball.gif
    # research_question: <strong>Research question:</strong> How can we develop a sim-to-real visual whole-body control framework for humanoid loco-manipulation?
    # key_features: <strong>Key features:</strong> Visuomotor policies; Whole-body dexterity; In-the-wild deployment; <strong>Humanoid loco-manipulation</strong>.
  
  - title: "RLVR-World: Training World Models with Reinforcement Learning"
    authors: Jialong Wu, <strong>Shaofeng Yin</strong>, Ningya Feng, Mingsheng Long†
    conference_short: Neurlps
    conference: Conference on Neural Information Processing Systems <strong>(Neurlps)</strong>, 2025.
    arxiv: https://arxiv.org/abs/2505.13934
    pdf: https://arxiv.org/pdf/2505.13934
    code: https://github.com/thuml/RLVR-World
    page: https://thuml.github.io/RLVR-World/
    image: ./assets/pub/arxiv2025_rlvr-world/RLVR_fast.gif
    # research_question: <strong>Research question:</strong> How can we train world models to better serve downstream tasks, beyond mere transition modeling?
    # key_features: <strong>Key features:</strong> RLVR training; strong improvements on language and video world models across text games, web navigation, and <strong>robot manipulation</strong>.

  - title: "Trajectory World Models for Heterogeneous Environments"
    authors: <strong>Shaofeng Yin*</strong>, Jialong Wu*, Siqiao Huang, Xingjian Su, Xu He, Jianye Hao, Mingsheng Long†
    conference_short: ICML
    conference: International Conference on Machine Learning (<strong>ICML</strong>), 2025.
    arxiv: https://arxiv.org/abs/2502.01366
    pdf: https://arxiv.org/pdf/2502.01366
    code: https://github.com/thuml/TrajWorld
    image: ./assets/pub/arxiv2025_trajworld/trajworld.png
    # research_question: <strong>Research question:</strong> How can we build generalizable sensor-based world models across diverse environments?
    # key_features: <strong>Key features:</strong> pretrained proprioceptive world model; single model for all robots; strong transferability; significant improvements in  MPC and OPE.

  - title: "iVideoGPT: Interactive VideoGPTs are Scalable World Models"
    authors: Jialong Wu*, <strong>Shaofeng Yin*</strong>, Ningya Feng, Xu He, Dong Li, Jianye Hao, Mingsheng Long†
    conference_short: Neurlps
    conference: Conference on Neural Information Processing Systems <strong>(Neurlps)</strong>, 2024.
    pdf: https://arxiv.org/pdf/2405.15223.pdf
    code: https://github.com/thuml/iVideoGPT
    page: https://thuml.github.io/iVideoGPT/
    image: ./assets/pub/nips2024_ivideogpt/oxe-standford-maskvit-14.gif
    # research_question: <strong>Research question:</strong> How can we leverage the advancements in scalable video generative models for developing interactive visual world models?
    # key_features: <strong>Key features:</strong> pretrained visual world model; unified model for diverse robot arms; strong transferability; high efficiency; significant improvements across MPC and MBRL on <strong>manipulation</strong> tasks.
